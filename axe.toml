# =============================================================================
# AXE Configuration File
#   Following subsections are available
#   APP
#   LOGGER - output setting
#   IO - base directory for IO
#   LSM - Log structured merge tree assumptions and settings
#   JOB - all job specific settings
#   LCM - Learned cost model specifics
#   LTune - Learned tuner specifics
#   SCHEDULERS - ML learning rate schduler kwargs
#   OPTIMIZERS - ML optimizer kwargs
#   LOSS - ML Loss function kwargs
# =============================================================================

# =============================================================================
# HEADER APP
#   Logic of app including jobs list to run
# =============================================================================
[app]
name = "AXE"
run = [
    # "create_lcm_data",
    # "train_lcm",
    # "create_ltuner_data",
    # "train_ltuner"
    "run_experiments",
]
random_seed = 2169
disable_tqdm = false

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "INFO"

# =============================================================================
# HEADER IO
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[io]
data_dir = "data"
database = "axe_vldb_mlos_with_init.db"

# =============================================================================
# HEADER LSM
#   Generic LSM settings
# =============================================================================
[lsm]
# Policy will effect everything else down stream (e.g. choice of neural network
# architecture for learned cost model)
#   Tiering
#   Leveling
#   Classic - Considers both leveing and tiering
#   QHybrid - Levels 1 -> L = Q
#   Fluid - Levels 1 -> (L-1) = Q, Level L = Z
#   Kapacity - Each level has own K_i decision
policy = 'Kapacity'

[lsm.bounds]
max_considered_levels = 20                  # Max number of levels to consider
size_ratio_range = [2, 22]                  # low, high of size ratio range
page_sizes = [4, 8, 16]                     # page size in KB
entry_sizes = [1024, 2048, 4096, 8192]      # avg. size of each entry bits
memory_budget_range = [5, 20]               # low, high, bits per element
selectivity_range = [1e-7, 1e-9]            # low, high
elements_range = [100000000, 2000000000]    # element range

# =============================================================================
# HEADER JOB
#   Settings for each individual job (executable)
# =============================================================================
# --------------------------------------
[job] # Common settings across all jobs
# --------------------------------------
use_gpu_if_avail = true

# --------------------------------------
[job.create_lcm_data]
# --------------------------------------
output_dir = "data/lcm/train_data/kaplsm"
num_samples = 65536
num_files = 16
num_workers = 8
overwrite_if_exists = false

# --------------------------------------
[job.train_lcm]
# --------------------------------------
max_epochs = 100
save_dir = "data/lcm/models/kaplsm_250523_1406"
no_checkpoint = false

data_split = 0.9
data_dir = "data/lcm/train_data/kaplsm"
batch_size = 128
shuffle = true
num_workers = 4

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
#   Huber - Huber loss
loss_fn = "MSE"

# Supported optimizers
#   [SGD, Adam, Adagrad, AdamW]
optimizer = "AdamW"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "CosineAnnealing"

# --------------------------------------
[job.create_ltuner_data]
# --------------------------------------
output_dir = "data/ltuner/train_data/std"
num_samples = 65536
num_files = 32
num_workers = 8
overwrite_if_exists = false

# --------------------------------------
[job.train_ltuner]
# --------------------------------------
max_epochs = 100
save_dir = "data/ltuner/models/kaplsm_250523_1406"
# Learned cost model is our loss, input full path to checkpoint or model file
loss_fn_path = "data/lcm/models/kaplsm_250523_1406"
optimizer = "AdamW"
lr_scheduler = "CosineAnnealing"
no_checkpoint = false

data_split = 0.9
data_dir = "data/ltuner/train_data/std"
batch_size = 1024
shuffle = true
num_workers = 4

# -----------------------------------------------------------------------------
[job.run_experiments]
# -----------------------------------------------------------------------------
exp_list = [
    # "ExperimentMLOS",
    "ExpLCMEvaluate",
    "ExpLTunerEvaluate",
]

# =============================================================================
# HEADER EXPERIMENTS
#   Configuration for experiments
# =============================================================================
[experiments]
lcm_path = "data/lcm/models/kaplsm_250514_1230"
lcm_model = "best_model.model"
ltuner_path = "data/ltuner/models/kaplsm_250514_1230"
ltuner_model = "best_model.model"
if_table_exists = "replace" # ['replace', 'fail', 'append']

# =============================================================================
# HEADER LCM
#   Add configurations related to learned cost models
# =============================================================================

# -----------------------------------------------------------------------------
# HEADER LCM.MODELS
#   Configuration for specific models
# -----------------------------------------------------------------------------
[lcm.model]
embedding_size = 8
hidden_length = 3
hidden_width = 64
decision_dim = 64
dropout = 0.0           # dropout percentage
norm_layer = "Batch"    # "Batch" or "Layer" norm

# Used only for classic models, generally smaller than embedding size
policy_embedding_size = 4

# =============================================================================
# HEADER LTUNE
#   Learned tuner module
# =============================================================================
[ltuner]
penalty_factor = 10

# kwargs specific to LTune models during forward pass
[ltuner.train_kwargs]
temp = 10
hard = false

[ltuner.validate_kwargs]
temp = 0.001
hard = true

# -----------------------------------------------------------------------------
# HEADER LTUNER.MODEL
#   Model configurations
# -----------------------------------------------------------------------------
[ltuner.model]
hidden_length = 4
hidden_width = 64
dropout = 0.1                     # dropout percentage
norm_layer = "Batch"            # batch or layer norm
categorical_mode = "gumbel"    # reinmax or gumbel

# =============================================================================
# HEADER SCHEDULERS
#   Specific settings for any learning rate schedulers
# =============================================================================
[scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0.0001  # minimum learning rate

[scheduler.Exponential]
gamma = 0.9

# =============================================================================
# HEADER OPTIMIZERS
#   Settings for any optimizers
# =============================================================================
[optimizer.Adam]
lr = 0.0001

[optimizer.AdamW]
lr = 0.001

[optimizer.SGD]
lr = 0.001

[optimizer.Adagrad]
lr = 0.001

# =============================================================================
# HEADER LOSS
#   Settings for individual loss functions
# =============================================================================
[loss.Huber]
reduction = 'sum'
delta = 10

[loss.MSE]
reduction = 'mean'
